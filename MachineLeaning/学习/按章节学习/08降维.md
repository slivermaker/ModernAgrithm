
**第15章降维算法**

将原始高维空间中的数据点映射到低维度的空间中，本章将从原理和实践的角度介绍两种经典的降维算法——线性判别分析和主成分分析。

 **15.1线性判别分析**

 　　线性判别式分析（Linear Discriminant Analysis,LDA），也叫作Fisher线性判别（Fisher Linear Discriminant,FLD），最开始用于处理机器学习中的分类任务，但是由于其对数据特征进行了降维投影，使其成为一种经典的降维方法。

**15.1.1降维原理概述**

 　　线性判别分析属于有监督学习算法，也就是数据中必须要有明确的类别标签，它不仅能用来降维，还可以处理分类任务
　　![](imge/md-20240306190140.png)

　　从投影结果上观察，图15-2（a）中的数据点经过投影后依旧有一部分混在一起，区别效果有待提高。图15-2（b）中的数据点经过投影后，没有混合，区别效果比图15-2（a）更好一些。因此，我们当然会选择图15-2（b）所示的降维方法，由此可见，降维不仅要压缩数据的特征，还需要寻找最合适的方向，使得压缩后的数据更有利用价值。

　　由图15-2可知，线性判别分析的原理可以这样理解：任务目标就是要找到最合适的投影方向，这个方向可以是多维的。

　　为了把降维任务做得更圆满，提出了两个目标。

　　　　1.对于不同类别的数据点，希望其经过投影后能离得越远越好，也就是两类数据点区别得越明显越好，不要混在一起。

　　　　2.对于同类别的数据点，希望它们能更集中，离组织的中心越近越好。

　　接下来的任务就是完成这两个目标，这也是线性判别分析的核心优化目标，降维任务就是找到能同时满足这两个目标的投影方向。

**15.1.2优化的目标**

 　　投影就是通过矩阵变换的方式把数据映射到最适合做分类的方向上：

![](imge/md-20240306190252.png)
　　由式（15.3）可以得到投影后的中心点计算方法，按照之前制定的目标，对于一个二分类任务来说，应当使得这两类数据点的中心离得越远越好，这样才能更好地区分它们：
![](imge/md-20240306191928.png)
　　现在可以把当作目标函数，目标是希望其值能够越大越好，但是只让不同类别投影后的中心点越远可以达到我们期望的结果吗？

　　对于图15-3所示的样本数据，假设只能在x<sub>1</sub>和x<sub>2</sub>两方向进行投影，如果按照之前定义的J<sub>(w)</sub>，显然x<sub>1</sub>方向更合适，但是投影后两类数据点依旧有很多重合在一起，而x<sub>2</sub>方向上的投影结果是两类数据点重合较少；因此，x<sub>2</sub>方向更好。

　　这个问题就涉及要优化的另一个目标，不仅要考虑不同类别之间要区分开，还要考虑同类样本点应当尽可能聚集在一起。显然在图15-3中，x<sub>1</sub>方向不满足这个条件，因为在x<sub>1</sub>方向上，同类样本变得更分散，不够集中。


　　![图15-3 投影方向选择](imge/md-20240306192117.png)

　　我们还可以使用另一个度量指标——散列值（scatter），表示同类数据样本点的离散程度，定义如下：
![](imge/md-20240306192144.png)
　　其中，y表示经过投影后的数据点，从式（15.5）中可以看出，散列值表示样本点的密集程度，其值越大，表示越分散；反之，则越集中。定义好要优化的两个目标后，接下来就是求解了。

**15.1.3线性判别分析求解**

 　　上一小节已经介绍了降维后想要得到的目标，现在把它们综合在一起，但是优化的目标有两个，那么如何才能整合它们呢？

　　既然要最大化不同类别之间的距离，那就把它当作分子；最小化同类样本之间的离散程度，那就把它当作分母，最终整体的J(W)依旧求其极大值即可。
![](imge/md-20240306200627.png)
　　在公式推导过程中，牢记最终的要求依旧是寻找最合适的投影方向，先把散列值公式展开：
![](imge/md-20240306200710.png)
　　观察一下式（15.15），它与线性代数中的特征向量有点像，如果把**S**<sub>w</sub><sup>-1</sup>当作一个整体，那么w就是其特征向量，问题到此迎刃而解。在线性判别分析中，其实只需要得到类内和类间散布矩阵，然后求其特征向量，就可以得到投影方向，然后，只需要对数据执行相应的矩阵变换，就完成全部降维操作。

**15.1.4Python实现线性判别分析降维**
见其他文件夹

**15.2主成分分析**

 　　主成分分析（Principal Component Analysis，PCA）是在降维中使用特别广泛的算法。在使用主成分分析降维的时候没有束缚，不像线性判别分析，必须要有数据标签，**只要拿到数据，没有标签也可以用主成分分析进行降维。所以应该先有一个直观的认识，主成分分析本质上属于无监督算法**，这也是它流行的主要原因。


物理意义会变
**15.2.1PCA降维基本知识点**

 　　既然不需要标签，就很难去分析类间与类内距离因素，那么该怎么办呢？PCA的基本思想就是方差，可以想象一下哪些特征更有价值？应当是那些区别能力更强的特征。例如我们想比较两个游戏玩家的战斗力水平。第一个特征是其所在帮派等级：A玩家，5级帮派；B玩家，4级帮派，A、B玩家的帮派等级看起来差别不大。第二个特征是其充值金额：A玩家，10000；B玩家，100。A、B玩家的充值金额的差距好像有些大。通过这两个特征就可以预估一下哪个玩家战斗力更强，答案肯定是A玩家。

　　现在再来观察一下这两个特征，帮派等级似乎相差不大，不能拉开差距，但是充值金额的差异却很大。我们希望得到充值金额这种能把不同玩家区分开的特征。在数学上可以用方差来描述这种数据的离散程度，所以在主成分析中主要依靠方差。

　　为了让大家更好地理解主成分分析，下面介绍一些基本概念。
![](imge/md-20240315142023.png)

　　（1）向量的表示。假设有向量（3，2），如图15-7所示。为什么向量可以表示为（3，2），这是在直角坐标系中的表示，如果坐标系变了，向量的表示形式也要发生变换。

　　　实际上该向量可以表示成线性组合3·(1，0)T+2·(0，1)T，其中（1，0）和（0，1）就称为二维空间中的一组基。

　　（3）基变换。大家常见的坐标系都是正交的，即内积为0，两两相互垂直，并且线性无关。为什么基都是这样的呢？如果不垂直，那么肯定线性相关，能用一个表示另一个，此时基就会失去意义，所以基的出发点就是要正交。


　　图15-7 向量的组成

　　基也可以进行变换，将一个向量从一组基变换到另一组基中。例如新的坐标系的两个基分别是
![](imge/md-20240315142609.png)
因此向量（3，2）映射到这个新的坐标系中，可以通过下面变换实现：
![](imge/md-20240315142516.png)
![](imge/md-20240315143239.png)

　　（3）方差和协方差。方差（variance）相当于特征辨识度，其值越大越好。协方差（covariance）就是不同特征之间的相关程度，协方差的计算式式为：

![](imge/md-20240315142629.png)

如果两个变量的变化趋势相同，例如随着身高的增长，体重也增长，此时它们的协方差值就会比较大，表示正相关。而方差又描述了各自的辨识能力，接下来就要把这些知识点穿插在一起。


**15.2.2PCA优化目标求解**
               ![](https://img-blog.csdn.net/20160718102051683?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

可知:

![](https://img-blog.csdn.net/20160718102125887?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

所以，确定了特征值之后，向量x的变换为：                         

![](https://img-blog.csdn.net/20160718102147459?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)  

引用《线性代数的几何意义》的描述：“矩阵乘法对应了一个变换，是把任意一个向量变成另一个方向或长度都大多不同的新向量。在这个变换的过程中，原向量主要发生旋转、伸缩的变化。如果矩阵对某一个向量或某些向量只发生伸缩变换，不对这些向量产生旋转的效果，那么这些向量就称为这个矩阵的特征向量，伸缩的比例就是特征值。”




 　　对于降维任务，无非就是将原始数据特征投影到一个更合适的空间，结合基的概念，这就相当于由一组基变换到另一组基，变换的过程要求特征变得更有价值，也就是方差能够更大。所以现在已经明确基本目标了：找到一组基，使得变换后的特征方差越大越好。

　　假设找到了第一个合适的投影方向，这个方向能够使得方差最大，对于降维任务来说，一般情况下并不是降到一维，接下来肯定要找方差第二大的方向。方差第二大的方向理论上应该与第一方向非常接近，甚至重合，这样才能保证方差最大，如图15-8所示。

　　在这种情况下，看似可以得到无数多个方差非常大的方向，但是想一想它们能组成想要的基吗？不能，因为没有满足基的最基本要求——线性无关，也就是相互垂直正交。所以在寻找方差最大的方向的同时，还要使得各个投影方向能够正交，即协方差应当等于0，表示完全独立无关。所以在选择基的时候，一方面要尽可能地找方差的最大方向，另一方面要在其正交方向上继续寻找方差第二大的方向，以此类推。
![](imge/md-20240315143319.png)
![](imge/md-20240315143530.png)


　　解释PCA中要求解的目标后，接下来就是在数学上将它表达出来。先来看一下协方差矩阵，为了简便，可以把数据中各个特征的均值默认为0，也可以认为数据已经进行过标准化处理。其计算式如下：
![](imge/md-20240315143821.png)
　　其中，X为实际的数据。包含2个特征a和b，一共有m个样本。

　　此时协方差矩阵为：
![](imge/md-20240315143832.png)
　　先观察一下协方差矩阵结果，其主对角线上的元素就是各个特征的方差（均值为0时），而非主对角线的上元素恰好是特征之间的协方差。按照目标函数的要求，首先应当使得方差越大越好，并且确保协方差为0，这就需要对协方差矩阵做对角化。
![](imge/md-20240315143906.png)
![](imge/md-20240315144039.png)
　　从一个n行n列的实对称矩阵中一定可以找到n个单位正交特征向量E=(e1,e2 ,… ,en)，以完成对角化的操作：
![](imge/md-20240315144146.png)


　　式（15.21）中的协方差矩阵恰好满足上述要求。假设需要将一组N维向量降为K维（K大于0，小于N），目标是选择K个单位正交基，使原始数据变换到这组基上后，各字段两两间协方差为0，各字段本身的方差尽可能大。当得到其协方差矩阵后，对其进行对角化操作，即可使得除主对角线上元素之外都为0。

　　其中对角线上的元素就是矩阵的特征值，这与线性判别分析很像，还是先把特征值按从大到小的顺序进行排列，找到前K个最大特征值对应的特征向量，接下来就是进行投影变换。

　　按照给定PCA优化目标，基本流程如下。

-   第①步：数据预处理，只有数值数据才可以进行PCA降维。
-   第②步：计算样本数据的协方差矩阵。
-   第③步：求解协方差矩阵的特征值和特征向量。
-   第④步：将特征值按照从大到小的顺序排列，选择其中较大的K个，然后将其对应的K个特征向量组成投影矩阵。
-   第⑤步：将样本点投影计算，完成PCA降维任务。

我们先来看它的定义，定义本身很简单，假设我们有一个n阶的矩阵A以及一个实数λ，使得我们可以找到一个非零向量x，满足：

![](imge/md-20240315144749.png)

如果能够找到的话，我们就称λ是矩阵A的特征值，非零向量x是矩阵A的特征向量。
![](imge/md-20240315144243.png)
[特征值与特征向量](https://zhuanlan.zhihu.com/p/104980382)


引用自
## 特征值和特征向量


[机智的博子](https://blog.csdn.net/fuming2021118535 "机智的博子") ![](https://csdnimg.cn/release/blogv2/dist/pc/img/newCurrentTime2.png) 于 2016-05-07 20:38:11 发布

在刚开始学的特征值和特征向量的时候只是知道了定义和式子，并没有理解其内在的含义和应用，这段时间整理了相关的内容，跟大家分享一下；

首先我们先把特征值和特征向量的定义复习一下：

定义： 设A是n阶矩阵，如果数λ和n维非零向量x使关系式

![](https://img-blog.csdn.net/20160718101611619?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)……(1)  

成立，那么，这样的数λ称为矩阵A的特征值，非零向量x称为A的对应于特征值λ的特征向量，（1）式还可以写为：

               ![](https://img-blog.csdn.net/20160718101749562?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)……（2）

如果想求出矩阵对应的特征值和特征向量就是求式（2）的解了。

那么，问题来了，这个式子要怎么理解呢？

首先得先弄清矩阵的概念:一个矩阵代表的是一个线性变换规则，而一个矩阵的乘法运行代表的是一个变换;

比如有一个矩阵A：

![](https://img-blog.csdn.net/20160718101812219?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

一个列向量为X为：

![](https://img-blog.csdn.net/20160718101823386?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

一个矩阵的乘法为：

![](https://img-blog.csdn.net/20160718101902174?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center) 

向量X通过矩阵A这个变化规则就可以变换为向量Y了

                       ![](https://img-blog.csdn.net/20160718101937503?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

在几何上的变换就类似于这样：

               ![](https://img-blog.csdn.net/20160718102030051?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

       知道了这个就可以从几何上理解特征值和特征向量是什么意思了，由  

               ![](https://img-blog.csdn.net/20160718102051683?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

可知:

![](https://img-blog.csdn.net/20160718102125887?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

所以，确定了特征值之后，向量x的变换为：                         

![](https://img-blog.csdn.net/20160718102147459?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)  

引用《线性代数的几何意义》的描述：“矩阵乘法对应了一个变换，是把任意一个向量变成另一个方向或长度都大多不同的新向量。在这个变换的过程中，原向量主要发生旋转、伸缩的变化。如果矩阵对某一个向量或某些向量只发生伸缩变换，不对这些向量产生旋转的效果，那么这些向量就称为这个矩阵的特征向量，伸缩的比例就是特征值。”

那么这样定义的特征值和特征向量有什么实际用途呢?在这里我举个数据挖掘算法中重要的一个算法：PCA（主成分分析）来给大家直观的感受一下。

首先，理解一下信息量这个概念

看几张图：

                     ![](https://img-blog.csdn.net/20160718102203538?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

如果我们单独看某一个维度的话，比如看x1这个维度

                      ![](https://img-blog.csdn.net/20160718102217210?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

可以看到将点投影到x1这个维度上看的话，图1的数据离散性最高，图3较低，图2数据离散性是最低的。数据离散性越大，代表数据在所投影的维度上具有越高的区分度，这个区分度就是信息量。如果我们用方差来形容数据的离散性的话，就是数据方差越大，表示数据的区分度越高，也就是蕴含的信息量是越大的。

         基于这个知识，如果我们想对数据进行降维的话，比如图1的两个维度的数据降成一维，我们可以选择保留X1这个维度的数据，因为在这个维度上蕴含的信息量更多。

同理，图2就可以保留x2这个维度的数据。但是，问题来了，图3应该保留哪个维度的数据呢？答案是保留哪个维度都不好，都会丢失较大的信息量。但是，如果我们把图3的坐标轴旋转一下

               ![](https://img-blog.csdn.net/20160718102232898?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)比较容易看出，图3在新的坐标轴下就能进行降维了。所以选取正确的坐标轴，然后根据各个维度上的数据方差大小，决定保留哪些维度的数据，这样的做法就是主成分分析的核心思想。

选取正确的坐标轴的过程中，我们需要一个矩阵变换，就类似于这样：   

                                           ![](https://img-blog.csdn.net/20160718102249680?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)  

也就是：

![](https://img-blog.csdn.net/20160718102303497?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)  

其实，经过数学上的推导的，我们就可以知道，特征值对应的特征向量就是理想中想取得正确的坐标轴，而特征值就等于数据在旋转之后的坐标上对应维度上的方差。

也就是说，直接求出矩阵A的特征向量得出对应的特征向量。我们就能找到旋转后正确的坐标轴。这个就是特征值和特征向量的一个实际应用：“得出使数据在各个维度区分度达到最大的坐标轴。”

所以，在数据挖掘中，就会直接用特征值来描述对应特征向量方向上包含的信息量，而某一特征值除以所有特征值的和的值就为：该特征向量的方差贡献率（方差贡献率代表了该维度下蕴含的信息量的比例）。

通常经过特征向量变换下的数据被称为变量的主成分，当前m个主成分累计的方差贡献率达到一个较高的百分数（如85%以上）的话，就保留着这m个主成分的数据。实现了对数据进行降维的目的。整个主成分分析的算法原理也就是这个。

**15.2.3Python实现PCA降维**
见另一个文件夹