![](imge/md-20240201220030.png)
![](imge/md-20240201220739.png)
![](imge/md-20240201221311.png)

## 熵
　总结上面所提到的问题，归根到底就是什么特征能够把数据集划分得更好，也就是哪个特征最好用，就把它放到最前面，因为它的效果最好，当然应该先把最厉害的拿出来。就像是参加比赛，肯定先上最厉害的队员（决策树中可没有田忌赛马的故事）。那么数据中有那么多特征，怎么分辨其能力呢？这就需要给出一个合理的判断标准，对每个特征进行评估，得到一个合适的能力值。

　　熵指物体内部的混乱程度，怎么理解混乱程度呢？可以分别想象两个场景：第一个场景是，当你来到义乌小商品批发市场，市场里有很多商品，看得人眼花缭乱，这么多商品，好像哪个都想买，但是又比较纠结买哪个，因为可以选择的商品实在太多。

　　根据熵的定义，熵值越高，混乱程度越高。这个杂货市场够混乱吧，那么在这个场景中熵值就较高。但是，模型是希望同一类别的数据放在一起，不同类别的数据分开。那么，如果各种类别数据都混在一起，划分效果肯定就不好，所以熵值高意味着数据没有分开，还是混杂在一起，这可不是模型想要的。

　　第二个场景是当你来到一个苹果手机专卖店，这一进去，好像没得选，只能买苹果手机。这个时候熵值就很低，因为这里没有三星、华为等，选择的不确定性就很低，混乱程度也很低。

　　如果数据划分后也能像苹果专卖店一样，同一类别的都聚集在一起，就达到了分类的目的，解释过后，来看一下熵的公式：
![](imge/md-20240201222154.png)
　再来说一说抛硬币的事，把硬币扔向天空后落地的时候，结果基本就是对半开，正反各占50%，这也是一个二分类任务最不确定的时候，由于无法确定结果，其熵值必然最大。但是，如果非常确定一件事发生或者不发生时，熵值就很小，熵值变化情况如图7-4所示。
![](imge/md-20240201222430.png)
![](imge/md-20240201223332.png)

![](imge/md-20240201222855.png)
![](imge/md-20240201223114.png)
![](imge/md-20240202143550.png)
![](imge/md-20240202142944.png)
举个例子用id分割的话每个集合都是一个个体，此时信息增益最大，但是没有意义，所以引进信息增益率将信息增益再除以自身熵值，比如len(id)=n  就是再除以abs(log(1/n))：
　在决策树算法中，经常会看到一些有意思的编号，例如ID3、C4.5，相当于对决策树算法进行了升级。基于信息增益的构建方法就是ID3，那么还有哪些问题需要改进呢？可以想象这样一种特征，样本编号为ID，由于每一个样本的编号都是唯一的，如果用该特征来计算信息增益，可能得到的结果就是它能把所有样本都分开，因为每一个节点里面只有一个样本，此时信息增益最大。但是类似ID这样的特征却没有任何实际价值，所以需要对信息增益的计算方法进行改进，使其能够更好地应对属性值比较分散的类似ID特征。

　　为了避免这个不足，科学家们提出了升级版算法，俗称C4.5，使用信息增益比率（gain ratio）作为选择分支的准则去解决属性值比较分散的特征。“率”这个词一看就是要做除法，再来看看ID这样的特征，由于取值可能性太多，自身熵值已经足够大，如果将此项作为分母，将信息增益作为分子，此时即便信息增益比较大，但由于其自身熵值更大，那么整体的信息增益率就会变得很小。


### 连续问题
上一小节使用的是离散属性的特征，如果数据是连续的特征该怎么办呢？例如对于身高、体重等指标，这个时候不仅需要找到最合适的特征，还需要找到最合适的特征切分点。在图7-1所示例子中，也可以按照年龄30岁进行划分，这个30也是需要给定的指标，因为不同的数值也会对结果产生影响。

　　如何用连续特征x=[60,70,75,85,90,95,100,120,125,220]选择最合适的切分点呢？需要不断进行尝试，也就是不断二分的过程，这里不是经典算法里二分而是不断找分割点进行测试
数据x一共有9个可以切分的点，需要都计算一遍，这一过程也是连续值的离散化过程。对于每一个切分点的选择，都去计算当前信息增益值的大小，最终选择信息增益最大的那个切分点，当作实际构建决策树时选择的切分点。

　　在这样一份数据中，看起来一一尝试是可以的，但是，如果连续值数据过于庞大怎么办呢？也可以人为地选择合适的切分点，并不是非要遍历所有的数据点。例如，将数据集划分成N块，这就需要遍历N次，其实无非就是效率问题，如果想做得更完美，肯定需要更多的尝试，这些都是可以控制的。

### 回归任务
 　　熵值可以用来评估分类问题，那么决策树是不是只能做分类任务呢？当然不止如此，回归任务照样能解决，只需要将衡量标准转换成其他方法即可。

在划分数据集时，回归任务跟分类任务的目标相同，肯定还是希望类似的数值划分在一起，例如，有一批游戏玩家的充值数据\[100,150,130,120,90,15000,16000,14500,13800\]，有的玩家充得多，有的玩家充得少。决策树在划分时肯定希望区别对待这两类玩家，用来衡量不同样本之间差异最好的方法就是方差。在选择根节点时，分类任务要使得熵值下降最多，回归任务只需找方差最小的即可。

　　最终的预测结果也是类似，分类任务中，某一叶子节点取众数（哪种类别多，该叶子节点的最终预测类别就是多数类别的）；回归任务中，只需取平均值当作最后的预测结果即可。

　　分类任务关注的是类别，可以用熵值表示划分后的混乱程度；回归任务关注的则是具体的数值，划分后的集合方差越小，把同类归纳在一起的可能性越大。

 　　讨论了如何建立决策树，下面再来考虑另一个问题：如果不限制树的规模，决策树是不是可以无限地分裂下去，直到每个叶子节点只有一个样本才停止？在理想情况下，这样做能够把训练集中所有样本完全分开。此时每个样本各自占领一个叶子节点，但是这样的决策树是没有意义的，因为完全过拟合，在实际测试集中效果会很差。

所以，需要额外注意限制树模型的规模，不能让它无限制地分裂下去，这就需要对决策树剪枝。试想，小区中的树木是不是经常修剪才能更美观？决策树算法也是一样，目的是为了建模预测的效果更好，那么如何进行剪枝呢？还是需要一些策略。
 　　通常情况下，剪枝方案有两种，分别是预剪枝（Pre-Pruning）和后剪枝（Post-Pruning）。虽然这两种剪枝方案的目标相同，但在做法上还是有区别。预剪枝是在决策树建立的过程中进行，一边构建决策树一边限制其规模。后剪枝是在决策树生成之后才开始，先一口气把决策树构建完成，然后再慢慢收拾它。

　　（1）预剪枝。在构造决策树的同时进行剪枝，目的是限制决策树的复杂程度，常用的停止条件有树的层数、叶子节点的个数、信息增益阈值等指标，这些都是决策树算法的输入参数，当决策树的构建达到停止条件后就会自动停止。

　　（2）后剪枝。决策树构建完成之后，通过一定的标准对其中的节点进行判断，可以自己定义标准，例如常见的衡量标准：
![](imge/md-20240202151607.png)
　　式（7.4）与正则化惩罚相似，只不过这里惩罚的是树模型中叶子节点的个数。式中，C(T)为当前的熵值；Tleaf为叶子节点个数，要综合考虑熵值与叶子节点个数。分裂的次数越多，树模型越复杂，叶子节点也就越多，熵值也会越小；分裂的次数越少，树模型越简单，叶子节点个数也就越少，但是熵值就会偏高。最终的平衡点还在于系数（它的作用与正则化惩罚中的系数相同），其值的大小决定了模型的趋势倾向于哪一边。对于任何一个节点，都可以通过比较其经过剪枝后值与未剪枝前值的大小，以决定是否进行剪枝操作。

后剪枝做起来较麻烦，因为首先需要构建出完整的决策树模型，然后再一点一点比对。相对而言，预剪枝就方便多了，直接用各种指标限制决策树的生长，也是当下最流行的一种决策树剪枝方法。

　　现阶段在建立决策树时，预剪枝操作都是必不可少的，其中涉及的参数也较多，需要大量的实验来选择一组最合适的参数，对于后剪枝操作来说，简单了解即可。


接下来实战
转到另一个文件夹